#!/usr/bin/env python3

from backend.validator import validate_venv
validate_venv()

import requests, argparse, sys

from scraper.page_processor import PageProcessor
from scraper.article_scraper import GoogleScraper
from nlp.nlp import NLProcessor

from definitions import NEGATIVE_QUERIES

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=10)
    args = parser.parse_args()
    if args.narticles <= 0:
        print("Sorry, values <= 0 are not valid numbers for articles to be scraped\nExiting")
        sys.exit(1)
    if len(NEGATIVE_QUERIES) == 0:
        print("No negative queries have been supplied in .env\nExiting")
        sys.exit(1)
    from database.mock_database import DatabaseNegative

    scraper = GoogleScraper(verbose=True, log_prefix="    \033[1mscraper:\033[0m ")
    for i, query in enumerate(NEGATIVE_QUERIES):
        print(f"\033[1mquery {i+1}/{len(NEGATIVE_QUERIES)}\033[0m")
        urls = scraper.find_news_urls_for_query(query, args.narticles)
        for url in urls:
            print(f"\r\033[K    \033[1mfetching\033[0m {url}", end="")
            page = requests.get(url).text
            processor = PageProcessor(page)
            summarization = "".join(NLProcessor.summarize(processor.get_fulltext()))
            DatabaseNegative.insert((url, summarization, query))
        print()
    print("\nDONE")

