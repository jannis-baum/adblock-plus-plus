#!/usr/bin/env python3

from helpers.validator import validate_venv
validate_venv()

import sys, argparse, random

from definitions import NEGATIVE_QUERIES
from scraper.scrape_news import scrape_news_pages
from nlp.nlp import NLProcessor

def pairdistance(summary1, summary2):
    return NLProcessor.__get_word_vectors().wm_distance(summary1, summary2)

def sample_fitness(sample):
    #fitness of a sample is the mean pairwise distance between all summaries in the sample,
    #not double-counting the distances
    fitness = 0
    while len(sample) > 0:
        base = sample[0]
        sample = sample[1:]
        for element in sample:
            fitness += pairdistance(base, element)
    return fitness

def sample_randomsearch(dataset, samplesize, iterations):
    previous_fitness = 0
    previous_sample = list()
    for i in range(iterations):
        sample = random.rample(dataset, samplesize)
        fitness = sample_fitness(sample)
        if fitness > previous_fitness:
            previous_sample = sample
            previous_fitness = fitness
    return previous_sample


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=10)
    parser.add_argument("-c", "--cluster", action="store_true")
    args = parser.parse_args()
    #TODO: make clustering possible on its own and after scraping
    if args.cluster:
        print(pairdistance("I like apples, bananas and peas.", "I like apples, bananas and peas."))
        print(pairdistance("I like apples, bananas and peas.", "Fire sucks and the President decided against it today."))
    else:
        if args.narticles <= 0:
            print("Sorry, values <= 0 are not valid numbers for articles to be scraped\nExiting")
            sys.exit(1)
        if len(NEGATIVE_QUERIES) == 0:
            print("No negative queries have been supplied in .env\nExiting")
            sys.exit(1)
        from database.mock_database import DatabaseNegative

        def process_page(page_processor, url, query):
            if url in DatabaseNegative.get_sources(): return
            summary = NLProcessor.summarize(page_processor.get_fulltext())
            DatabaseNegative.insert(query, summary, url)
        scrape_news_pages(NEGATIVE_QUERIES, args.narticles, process_page)
    print("DONE")

