#!/usr/bin/env python3

from helpers.validator import validate_venv
validate_venv()

import sys, argparse, random

from definitions import NEGATIVE_QUERIES
from scraper.scrape_news import scrape_news_pages
from nlp.nlp import NLProcessor
from database.mock_database import DatabaseNegative

def pairdistance(summary1, summary2):
    return NLProcessor.__get_word_vectors().wm_distance(summary1, summary2)

def sample_fitness(sample):
    #fitness of a sample is the mean pairwise distance between all summaries in the sample,
    #not double-counting the distances
    fitness = 0
    while len(sample) > 0:
        base = sample[0]
        sample = sample[1:]
        for element in sample:
            fitness += pairdistance(base, element)
    return fitness

def sample_randomsearch(dataset, samplesize, iterations):
    previous_fitness = 0
    previous_sample = list()
    for i in range(iterations):
        sample = random.sample(dataset, samplesize)
        fitness = sample_fitness(sample)
        if fitness > previous_fitness:
            previous_sample = sample
            previous_fitness = fitness
    return previous_sample

def find_best_match(cluster_medians, summary):
    cluster_median_similarities = [pairdistance(median, summary) for median in cluster_medians]
    return cluster_medians[cluster_median_similarities.index(min(cluster_median_similarities))]


def create_clusters(samplesize, iterations):
    dataset = DatabaseNegative.get_all_summaries()
    cluster_medians = sample_randomsearch(dataset, samplesize, iterations)
    remaining_data = list(set(cluster_medians) - set(dataset))
    clusters = dict()
    for summary in remaining_data:
        match = find_best_match(cluster_medians, summary)
        if match in clusters:
            clusters[match].append(summary)
        else:
            clusters[match] = [summary]
    DatabaseNegative.insert_new_clusters(clusters)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=10)
    parser.add_argument("-c", "--cluster", action="store_true")
    parser.add_argument("-e", "--exclusive_clustering", action="store_true")
    parser.add_argument("-i", "--iterations", type=int, action="store", default=1000)
    parser.add_argument("-s", "--sample_size", type=int, action="store", default=5)
    args = parser.parse_args()
    
    if args.exclusive_clustering:
        create_clusters(args.sample_size, args.iterations)
    else:
        if args.narticles <= 0:
            print("Sorry, values <= 0 are not valid numbers for articles to be scraped\nExiting")
            sys.exit(1)
        if len(NEGATIVE_QUERIES) == 0:
            print("No negative queries have been supplied in .env\nExiting")
            sys.exit(1)

        def process_page(page_processor, url, query):
            if url in DatabaseNegative.get_sources(): return
            summary = NLProcessor.summarize(page_processor.get_fulltext())
            DatabaseNegative.insert(query, summary, url)
        scrape_news_pages(NEGATIVE_QUERIES, args.narticles, process_page)
        if args.cluster:
            create_clusters(args.sample_size, args.iterations)
    print("DONE")

