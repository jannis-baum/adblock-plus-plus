#!/usr/bin/env python3

from backend.validator import validate_venv
validate_venv()

import sys, argparse, requests

from definitions import *

from scraper.page_processor import PageProcessor
from scraper.article_scraper import GoogleScraper

from database.mock_database import DatabasePositive

from nlp.nlp import NLProcessor
from btm.script.infer import infer_file

TEMP_NORMALIZED_DOCS = os.path.join(TEMP_DIR, '.scraping_normalized_results.temp.txt')
TEMP_INFERRED_TOPICS = os.path.join(TEMP_DIR, '.scraping_inferred_vectors.temp.txt')

def write_temp_normalized(starting_at = 0):
    with open(TEMP_NORMALIZED_DOCS, 'w') as fp:
        fp.write('\n'.join([
            NLProcessor.normalize(doc)
        for doc in DatabasePositive.get_paragraphs()[starting_at:]]))

def save_temp_inferred():
    with open(TEMP_INFERRED_TOPICS, 'r') as fp:
        DatabasePositive.insert_vectors([
            [float(a.strip()) for a in vector_string.split()]
        for vector_string in fp.readlines()])

def scrape():
    urls = list()
    for query in POSITIVE_QUERIES:
        urls += GoogleScraper.find_news_urls_for_query(query, args.narticles)
    GoogleScraper.quit()
    for url in urls:
        page = requests.get(url).text
        processor = PageProcessor(page)
        paragraphs = processor.get_all_paragraphs()
        for paragraph in paragraphs:
            DatabasePositive.insert_paragraph(paragraph)
            
def train():
    from btm.script.train import train
    write_temp_normalized()
    indexed_docs_path = train(TEMP_NORMALIZED_DOCS, TOPIC_MODEL_PATH, TRAINING_TOPICS, TRAINING_ALPHA, TRAINING_BETA, TRAINING_ITERATIONS, TRAINING_SAVE_STEPS)
    os.remove(TEMP_NORMALIZED_DOCS)
    return indexed_docs_path

def main(n_articles, train):
    validate_venv()
    if train:
        DatabasePositive.clear_vectors()
        scrape()
        indexed_docs_path = train()
        print('INFERRING TOPICS ...')
        infer_file(TOPIC_MODEL_PATH, indexed_docs_path, TEMP_INFERRED_TOPICS, should_index=False)
        save_temp_inferred()
    else:
        try: BTMInferrer(TOPIC_MODEL_PATH) # verify integrity of model
        except:
            print(f'model missing, run {__file__} -t to train model')
            sys.exit(1)

        existing = DatabasePositive.count_paragraphs()
        scrape()
        write_temp_normalized(starting_at=existing)
        infer_file(TOPIC_MODEL_PATH, TEMP_NORMALIZED_DOCS, TEMP_INFERRED_TOPICS, should_index=True)

    print('DONE!')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=0)
    parser.add_argument("-t", "--train", action="store_true")
    args = parser.parse_args()

    main(args.narticles, train)

