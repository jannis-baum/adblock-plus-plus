#!/usr/bin/env python3

from backend.validator import validate_venv
validate_venv()

import sys, argparse

from definitions import *
from scraper.scrape_news import scrape_news_urls
from database.mock_database import DatabasePositive
from nlp.nlp import NLProcessor
from btm.script.infer import infer_file, BTMInferrer

TEMP_NORMALIZED_DOCS = os.path.join(TEMP_DIR, ".scraping_normalized_results.temp.txt")
TEMP_INFERRED_TOPICS = os.path.join(TEMP_DIR, ".scraping_inferred_vectors.temp.txt")


def write_temp_normalized(starting_at=0):
    with open(TEMP_NORMALIZED_DOCS, "w") as fp:
        fp.write(
            "\n".join(
                [
                    NLProcessor.normalize(doc)
                    for doc in DatabasePositive.get_paragraphs()[starting_at:]
                ]
            )
        )


def save_temp_inferred():
    with open(TEMP_INFERRED_TOPICS, "r") as fp:
        DatabasePositive.insert_vectors(
            [
                [float(a.strip()) for a in vector_string.split()]
                for vector_string in fp.readlines()
            ]
        )


def scrape(narticles):
    print("SCRAPING")
    def process_page(page_processor, url, query):
        paragraphs = page_processor.get_all_paragraphs()
        for paragraph in paragraphs:
            if any(c in paragraph for c in ["  ", "/", "\n", "_", "|", "\\_"]):
                continue
            DatabasePositive.insert_paragraph(paragraph)
    scrape_news_urls(POSITIVE_QUERIES, narticles, process_page)
    print("DONE")


def train():
    from btm.script.train import train

    write_temp_normalized()
    indexed_docs_path = train(
        TEMP_NORMALIZED_DOCS,
        TOPIC_MODEL_PATH,
        TRAINING_TOPICS,
        TRAINING_ALPHA,
        TRAINING_BETA,
        TRAINING_ITERATIONS,
        TRAINING_SAVE_STEPS,
    )
    os.remove(TEMP_NORMALIZED_DOCS)
    return indexed_docs_path


def main(n_articles, train):
    if train:
        DatabasePositive.clear_vectors()
        scrape(n_articles)
        indexed_docs_path = train()
        print("INFERRING TOPICS ...")
        infer_file(
            TOPIC_MODEL_PATH,
            indexed_docs_path,
            TEMP_INFERRED_TOPICS,
            should_index=False,
        )
        save_temp_inferred()
    else:
        try:
            BTMInferrer(TOPIC_MODEL_PATH)  # verify integrity of model
        except:
            print(f"model missing, run {__file__} -t to train model")
            sys.exit(1)

        existing = DatabasePositive.count_paragraphs()
        scrape(n_articles)
        write_temp_normalized(starting_at=existing)
        infer_file(
            TOPIC_MODEL_PATH,
            TEMP_NORMALIZED_DOCS,
            TEMP_INFERRED_TOPICS,
            should_index=True,
        )

    print("DONE!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=10)
    parser.add_argument("-t", "--train", action="store_true")
    args = parser.parse_args()

    if args.narticles <= 0:
        print("Sorry, values <= 0 are not valid numbers for articles to be scraped\nExiting")
        sys.exit(1)
    main(args.narticles, train)

