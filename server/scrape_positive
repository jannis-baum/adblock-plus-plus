#!/usr/bin/env python3

import requests, argparse

from scraper.page_processor import PageProcessor
from scraper.article_scraper import GoogleScraper

from btm.script.infer import BTMInferrer

from definitions import *

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=0)
    parser.add_argument("-t", "--train", action="store_true")
    args = parser.parse_args()
    from database.mock_database import DatabasePositive

    urls = list()
    for query in POSITIVE_QUERIES:
        urls += GoogleScraper.find_news_urls_for_query(query, args.narticles)
    GoogleScraper.quit()
    for url in urls:
        page = requests.get(url).text
        processor = PageProcessor(page)
        paragraphs = processor.get_all_paragraphs()
        for paragraph in paragraphs:
            DatabasePositive.insert_paragraph(paragraph)

    if args.train:
        from nlp.nlp import NLProcessor
        from btm.script.train import train
        docs = [
            NLProcessor.normalize(doc)
        for doc in DatabasePositive.get_sentences()]
        with open('.temp_docs.txt', 'w') as fp:
            fp.write('\n'.join(docs))
        train('.temp_docs.txt', TOPIC_MODEL_PATH, TRAINING_TOPICS, TRAINING_ALPHA, TRAINING_BETA, TRAINING_ITERATIONS, TRAINING_SAVE_STEPS)
        os.remove('.temp_docs.txt')

    inf = BTMInferrer(TOPIC_MODEL_PATH)
    print('INFERRING PARAGRAPHS')
    paragraphs = DatabasePositive.get_sentences()
    for n, paragraph in enumerate(paragraphs):
        print(f'\rINFERRING {n+1}/{len(paragraphs)}', end='')
        DatabasePositive.insert_vector(inf.infer(NLProcessor.normalize(paragraph)))
    print('\ndone!')

