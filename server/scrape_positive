#!/usr/bin/env python3

import requests, argparse

from scraper.page_processor import PageProcessor
from scraper.article_scraper import GoogleScraper

from definitions import POSITIVE_QUERIES


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-m", "--mock", action="store_true")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=0)
    args = parser.parse_args()
    if args.mock:
        from database.mock_database import DatabasePositive
    else:
        from database.database import DatabasePositive

    urls = list()
    for query in POSITIVE_QUERIES:
        urls += GoogleScraper.find_news_urls_for_query(query, args.narticles)
    GoogleScraper.quit()
    for url in urls:
        page = requests.get(url).text
        processor = PageProcessor(page)
        paragraphs = processor.get_all_paragraphs()
        for paragraph in paragraphs:
            DatabasePositive.insert((paragraph,))
