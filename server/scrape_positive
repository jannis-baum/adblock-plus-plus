#!/usr/bin/env python3

import sys, argparse, requests

from definitions import *

from scraper.page_processor import PageProcessor
from scraper.article_scraper import GoogleScraper

from database.mock_database import DatabasePositive

from nlp.nlp import NLProcessor
from btm.script.infer import BTMInferrer

def scrape():
    urls = list()
    for query in POSITIVE_QUERIES:
        urls += GoogleScraper.find_news_urls_for_query(query, args.narticles)
    GoogleScraper.quit()
    for url in urls:
        page = requests.get(url).text
        processor = PageProcessor(page)
        paragraphs = processor.get_all_paragraphs()
        for paragraph in paragraphs:
            DatabasePositive.insert_paragraph(paragraph)

def train():
    from btm.script.train import train
    docs = [
        NLProcessor.normalize(doc)
    for doc in DatabasePositive.get_sentences()]
    with open('.temp_docs.txt', 'w') as fp:
        fp.write('\n'.join(docs))
    train('.temp_docs.txt', TOPIC_MODEL_PATH, TRAINING_TOPICS, TRAINING_ALPHA, TRAINING_BETA, TRAINING_ITERATIONS, TRAINING_SAVE_STEPS)
    os.remove('.temp_docs.txt')

def main(n_articles, train):
    if train:
        scrape()
        train()
        inf = BTMInferrer(TOPIC_MODEL_PATH)
    else:
        try: inf = BTMInferrer(TOPIC_MODEL_PATH)
        except:
            print(f'model missing, run {__file__} -t to train model')
            sys.exit(1)
        scrape()

    print('INFERRING PARAGRAPHS')
    paragraphs = DatabasePositive.get_sentences()
    for n, paragraph in enumerate(paragraphs):
        print(f'\rINFERRING {n+1}/{len(paragraphs)}', end='')
        DatabasePositive.insert_vector(inf.infer(NLProcessor.normalize(paragraph)))
    print('\ndone!')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="scraper")
    parser.add_argument("-n", "--narticles", type=int, action="store", default=0)
    parser.add_argument("-t", "--train", action="store_true")
    args = parser.parse_args()

    main(args.narticles, train)

